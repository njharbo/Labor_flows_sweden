\section{Method}
We derive our decomposition method by first fully defining the assumed underlying stochastic process, and then show what moments from the stochastic process our method identifies.
This means an upfront cost in notation to clarify everything, but we think this is outweighed by the gain in being clear in all assumption underlying the estimation procedure.

[I will weaken the following formulation but internally I'll be clear with the gist]. Given that there is a large number of conflicting ways of doing this, and the literature has not yet settled on any standard method, it seems to be useful to show the whole working of an estimation procedure, starting with a fully parametrized stochastic process and go from there.

We will also be fully general in allowing for an arbitrary number of states. This clarifies the underlying logic of the estimation procedure for the general case.

\subsection{Environment and notation}
So we start by defining the stochastic process that we assume generate our data. We assume that workers can be in $S$ different states from the set $\cal S = \{1,\dots,S\}$. In our particular case, we will have $S=4$. Other studies has looked at $S=2$ and $S=3$ respectively.

Each individual jumps between states according to a flow matrix $Q(t)$. This means that an individual at time $t$ in state $s$ flows to state $s'$ according to a poisson process with rate $Q(t)_{s,s'}$. The diagonal elements on $Q(t)$ are such that each row sum to zero, i.e.
\[
	Q(t)_{s,s} = -\sum_{s' \neq s} Q(t)_{s,s'}.
\]

With this setup, workers are flowing between states according to a \emph{time-inhomogenous continuous time Markov chain generated by $Q(t)$}. The process is a continuous time Markov chain as it has this property of people flowing to other states at a poission rate. The process is a time-inhomogenous as the flow matrix $Q(t)$ changes over time.

We write $x(t)$ to denote the stochastic process defined above. It is fully defined by the flow matrix function $Q(t)$. It is often convenient to analyze the transition matrix $P$ between two arbitrary time periods as well. This is defined for $t<t'$ as
\[
P(t,t')_{s,s'} = \mathbb{P}(x(t')=s'|x(t)=s).
\]
It is a well-known theorem in continuous time markov chain theory that the transition matrix is given by
\[
P(t,t') = \exp\left(\int_{t}^{t'} Q(z)dz\right)
\]
where $\exp(\cdot)$ denotes matrix exponentiation. We can illustrate the intuition behind this equation for a two state system. For small $\delta$ we have
\begin{eqnarray*}
	\exp\left(\int_{t}^{t+\delta} Q(z)dz\right)&\approx & \exp\left(\delta Q(t)\right) \\
	&=& \left(\begin{array}{cc}
	1-\delta(Q(t)_{1,2}) & \delta Q(t)_{1,2}\\
	\delta Q(t)_{2,1} & 1-\delta Q(t)_{2,1}
\end{array}\right)	 \\
&\approx & P(t,t+\delta)
\end{eqnarray*}
The last equation is because we consider a small time interval $\delta$, and agents leave state $1$ at rate $Q(t)_{1,2}$ which becomes roughly $\delta Q(t)_{1,2}$ for small time intervals. By using that we obtain $P(t,t')$ by repeated multiplication we can let $\delta\to 0$ and obtain the integral.
\subsection{Expected unemployment share and law of large numbers **}
[tl;dr version of this: with this formulation of stochastic processes, the expected share of individuals in a particular state $\mathbb{E}\sigma(t)$, which is a vector of length $S$ (element $s$ gives the share of that particular element), satisfies the equation
\[
\mathbb{E}\sigma(t) = \mathbb{E}\sigma(0)P(0,t)
\]
i.e. we can think of expected value vectors of shares being transformed by the transition matrix. Therefore, going forward we will write $\sigma(t)$ for the expected share vector in different states, with initial share $\sigma(0)$ given. We will assume that expected share is equal to actual share, i.e. that sampling error causes negligible errors in measures shares compared to expected shares.]\\\\

When other papers do decomposition analyses, they usually neglect the sampling error which results as the share of workers in a particular state does not always equal the expected share of workers given the underlying stochastic process. This neglect can be justified if the sample is large by reference to the law of large numbers.

However, as we have the ambition of starting from a stochastic process and derive everything from there, we will be explicit with defining the shares in terms of the stochastic processes of individual workers, and then show exactly where we refer to the law of large numbers. This is also good to do because when we see where the law of large numbers is called upon, we can see whether it is justified in particular cases, which can be important if we subdivide the labor market into many states.

So let's get started.  Assume that there are $N$ workers and we write $x_{i}(t), i \in \{1,\dots, N\}$ for the stochastic process for each worker. Worker $i$ start in state $s_{0,i}$. The processes $x_{i}(t)$ follow the process $x(t)$ in the sense that
\[
\mathbb{P}(x_i(t) = s) = P(0,t)_{s_{0,i},s}
\]
i.e. the probability that worker $i$ is in state $s$ at time $t$ is given by the transition matrix $P(0,t)$ given that we start from state $s_{0,i}$.

With this formulation, the share of workers in state $s$ is
\[
\sigma_s(t)=\frac{\sum_{i=1}^n \mathbb{I}(x_{i}(t) = s)}{N}
\]
where $\mathbb{I}(\cdot)$ is the indicator function which takes value one if the condition is true and zero otherwise. The expected value of this share is given by
\begin{eqnarray*}
\mathbb{E}\sigma_s(t)&=&\frac{\sum_{i=1}^n \mathbb{E}\mathbb{I}(x_{i}(t) = s)}{N}\\
&=& \frac{\sum_{i=1}^n \mathbb{P}(x_{i}(t) = s)}{N}
\end{eqnarray*}
We want to re-express this in terms of the initial distribution of the initial state of workers and the transition matrix from $0$ to $t$. It is intuitive that this should be the case, as we have that the expected value "should" be the transition matrix times the initial probability distribution. We prove this by collecting terms which start on the same initial state $s_0$. Write $N_{s_0}$ for the number of individuals that start in state $s_0$ and note that
\[
\mathbb{E}\sigma_{s_0}(0) = \frac{N_{s_0}}{N}.
\]
i.e. the proportion of elements in each state. This equation just says that the expected share of workers in state $s_0$ at time $0$ is the proportion of workers in that state. As the initial state is deterministic, this is no problem.

With this formulation, we get 
\begin{eqnarray*}
	\mathbb{E}\sigma_s(t) &=& \frac{\sum_{i=1}^n \mathbb{P}(x_{i}(t) = s)}{N}\\
	&=&  \frac{\sum_{i=1}^n }{N}\\
	&=& \frac{\sum_{s_0 \in \cal S} \sum_{i\in \{1,\dots, N\}; s_{0,i}=s} P(0,t)_{s_{0,i},s}}{N}\\
	&=& \frac{\sum_{s_0 \in \cal S} P(0,t)_{s_{0,i},s} N_{s_0}}{N}\\
	&=& \sum_{s_0 \in \cal S} (\mathbb{E}\sigma_{s_0}(0)) P(0,t)_{s_{0,i},s}\\
	&=& \left(\mathbb{E}\sigma(0)P(0,t)\right)_s
\end{eqnarray*}

\subsection{Identifying the flow matrix $Q(t)$}
We observe the distribution vector $x(t) \in \mathbb{R}^S$ at discrete time points
\[
t = 0,\dots, T
\]
and we also observe the transition matrix $P(t,t+3)$ (here, the unit of time is one month, and we get the transition probabilities over three month periods). The aim of our analysis is to use our data to first estimate the flow matrix $Q(t)$. 

To do this, we use that
\[
P(t,t+3) = \exp\left(\int_{t}^{t+3} Q(z)dz\right)
\]
where $\exp\left(\dots\right)$ is matrix exponentiation. 

To identify $Q$ we further make the assumption that $Q(t)$ is constant on each measurement interval $[t,t+3)$. Under this assumption, we estimate $Q(t)$ by
\[
Q(t)=\frac{\log(P(t,t_3))}{3}
\]
Can we be sure that we just can take this matrix logarithm and get a unique answer? The answer is yes under some conditions. The matrix logarithm exists and is real-valued if and only if the negative eigenvalues of $P$ in the Jordan composition has an even block size. If this is not true, $P$ is not "embeddable" which means that it cannot be the result from a continuous time Markov chain with constant flow matrix. If this is the case there exist algorithms to derive an approximate generating flow matrix $Q$.
[When I write this I see that our assumption is very strange, as we disconfirm it when we use $P(t+1,t+4)$. Writing this actually makes me want to change it to something else, maybe that $Q(t)$ is linearly interpolated between measurement times or similarly].

\subsection{Stationary distribution approximation and the convergence rate to the stationary distribution}
In some cases, we can approximate the observed distribution $x(t)$ by the corresponding steady state distribution of the estimated $Q(t)$ for the interval $t\in[t-3,t)$ (we write $Q_t$ for this value going forward.

When $Q_t$ have $S$ distinct eigenvalues, it has on zero eigenvalue $\lambda_1$ and the rest of the eigenvalues have negative real parts:
\[
Re(\lambda_S)<Re(\lambda_{S-1})<\dots < Re(\lambda_1)=0.
\]
The eigenvector $\bar x_t$ corresponding to $\lambda_1$ is the unique stationary distribution provided that $\sum \bar x_t=1$ where $\sum$ denotes component-wise summation.

All other eigenvectors sum to zero. As the full share vector in the previous quarter $x(t-3)$ sums to $1$, we know that there must be a coefficient $1$ on the steady state eigenvector $\bar x_t$. Thus, we can write 
\[
x(t-3)=\bar x_t + \sum_{i=2}^S q^i_t x_t^i
\]
i.e. the stationary distribution has coefficient one and the others have arbitrary coefficients $q^i_s$. 

We can use this eigenvalue decomposition of $x(t-3)$ to place bounds on the distance between $x(t)$ and $\bar x_t$, i.e. between $x(t)$ and the corresponding steady state vector. The key is that $\exp(3Q_t)$ has eigenvalues $1,\exp(3\lambda_2),\dots,\exp(3\lambda_S)$ with the same eigenvectors as $Q$. Thus, we have
\[
\exp(3Q)x(t-3)=\bar x_t + \sum_{i=2}^S q^i_t \exp(\lambda_i)x_t^i.
\]
Thus, the terms which are not the steady state decay at rate $\lambda_i<0$. Thus, we can formulate the distance between them as
\begin{eqnarray*}
||x(t)-\bar x_t|| &=& ||x(t-3)\exp(3Q_t)-\bar x_t||\\
&=& ||\sum_{i=2}^S q^i_s \left(\exp(3Q^s)\right) x_s^i||\\
&\le& \sum_{i=2}^S |q^i_s| ||\exp(3\lambda_i)x_s^i||\\
&\le& \exp(3\lambda_2) \sum_{i=2}^S |q^i_s| ||x_s^i||
\end{eqnarray*}
So we see that the distance between $x(t)$ and its corresponding steady state is bounded above by $\exp(3\lambda_2)$ where $\lambda_2$ is the second largest eigenvalue of $Q_t$.\footnote{In some papers, the important term for convergence is $f+s$ where $f$ is job finding rate and $s$ is job separation rate. This is because in a two-dimensional model $\left(\begin{array}{cc} -s & s \\ f & -f\end{array}\right)$ is the transition matrix and $-(f+s)$ is the second largest eigenvalue.}

\subsection{Taylor approximation}
Insofar $\lambda_2$ is very negative and convergence is rapid, the sequence of observations can be closely approximated as a function only of $Q_t$ where $Q_t$ is the value $Q(t)$ takes on the interval $[t-3,t)$. 

So we obtain a sequence of observations steady states $f(Q_t)$ and want to derive how much of the variation in $f$ can be attributed to different components of $Q_t$.

We want to focus on cyclical variations in $Q_t$ and $f(Q_t)$ so we want to remove trends. Thus, we extract the trend $\bar{Q}_t$ and see how $f(Q_t)-f(\bar{Q}_t)$ is driven by the different components of $Q$. 

The natural way to do this is to do a Taylor expansion of $f(Q_t)-f(\bar{Q}_t)$ which allows us to linearly decompose. We do a log-linear transformation and obtain
\[
\log\left(\frac{f(Q_t)}{f(\bar{Q}_t)}\right) = \sum_{i,j; i\neq j} \frac{\partial f}{\partial Q_{i,j}}\frac{(\bar{Q}_t)_{i,j}}{f}\log\left(\frac{(Q_t)_{i,j}}{(\bar{Q}_t)_{i,j}}\right)+\mathcal{O}\left(||Q_t-\bar{Q}_t||^2\right)
\]
With this formulation, we can define the percentage contribution of flow matrix element $Q_{i,j}$ as 
\[
\beta_{i,j} = \frac{Cov\left(\log\left(\frac{f(Q_t)}{f(\bar{Q}_t)}\right), \frac{\partial f}{\partial Q_{i,j}}\frac{(\bar{Q}_t)_{i,j}}{f}\log\left(\frac{({Q}_t)_{i,j}}{(\bar{Q}_t)_{i,j}}\right)\right)}{Var\left(\log\left(\frac{f(Q_t)}{f(\bar{Q}_t)}\right)\right)}
\]
and this will sum to 1 apart from the quadratic error term.


\subsection{Eigenvalue analysis and appropriateness of decomposition}
The convergence rate of a Markov chain to steady state can be analyzed using eigenvalue analysis. In the generic case, the flow matrix $Q_t$ (short-hand for the value $Q(t)$ takes for the interval $[t,t+s)$) has $n$ distinct eigenvalues $\lambda_1(t),\dots,\lambda_n(t)$ which satisfy
\[
\lambda_1 < \dots \lambda_{n-1}<\lambda_n = 0
\]
with corresponding eigenvectors $v_1(t),\dots,v_n(t)$. Here, $v_n(t)$ has only non-negative value and is the unique invariant probability distribution associated with $Q_t$ once we normalize its sum to $1$.

This means that $\{v_i(t)\}_{i=1}^n$ form a basis for $\mathbb{R}^n$. Thus, we can decompose the labor market state $x(t)$
\[
x(t)=\sum_{i=1}^n q_i(t)v_i(t)
\]
for some real values $q_i$. It can be shown that $q_n(t)=1$ for all $Q_t$ which means that the invariant probability distribution vector $v_n(t)$ always gets weight $1$ when we decompose the probability distribution $x(t)$ with respect to the flow matrix $Q_t$. Writing $\bar{x}(t)$ for this steady-state, we obtain
\[
x(t)=\bar{x}(t)+\sum_{i=1}^{n-1}q_i(t)v_i(t).
\]
Now, we use that if $v$ is a left eigenvector of $Q$ with eigenvalue $\lambda$, then $v$ is also an eigenvalue of $\exp(sQ)$ with eigenvalue $\exp(s\times\lambda)$. Indeed
\begin{eqnarray*}
v\exp(Q)&=&v\sum_{j=0}^\infty \frac{(sQ)^j}{j!}\\
&=&\sum_{j=0}^\infty \frac{s^j \lambda^j v}{j!}\\
&=&\exp(s\lambda)v
\end{eqnarray*}
as required. This means that we can use the decomposition to obtain
\begin{eqnarray*}
x(t+s)&=&x(t)exp(Q_t s)\\
&=&\bar{x}(t)+\sum_{i=1}^{n-1}q_i(t)v_i(t)\exp(sQ_t)\\
&=&\bar{x}(t)+\sum_{i=1}^{n-1}q_i(t)\exp(s\lambda_i)v_i(t)
\end{eqnarray*}
As $\exp(s\lambda_i)\to 0$ for $s\to\infty$ whenever $\lambda_i<0$ (i.e. for $i=1,\dots,n-1$), we obtain the classic result that $x(t+s)\to \bar{x}(t)$. Moreover, we get that that the convergence rate is determined by $\lambda_{n-1}$, the second largest eigenvalue (as all other terms become insignificant compared to the $\exp(\lambda_{n-1}s)$-term asymptotically. Thus, to analyze whether a steady-state approximation is justified we can analyze the second eigenvalue of the flow matrix.

\subsection{Eigenvalues in our data}
When we do an eigenvalue analysis of our estimated flow matrices, we get that the second largest eigenvalue is approximately $-0.02$. This means that the convergence rate is only $2\%$ monthly and $6\%$ on a quarterly basis. This means that a steady state analysis can be inappropriate with our data. (In standard two-state analysis in the US, the flow matrix is 
\[
Q=\left(\begin{array}{cc}
	  -s & s \\
	  f & -f\end{array}\right)
\]
which has a second largest eigenvalue $-(s+f)$ which has an approximate size $0.5$). [To be added: Redoing our analysis without permanent/temporary partition etc].

The main problem in the data is that the loss rate is extremely low from permanent jobs in Sweden, which means that it takes a long time for the economy to adjust to a new steady state. [Could be added here: "decompose" the causes of a large second eigenvalue].

\subsection{Decomposition without steady-state approximation}
In this section, we outline how to decompose the cyclical component of unemployment without using a steady-state approximation. 

We begin with some notation. Let $\lambda^{ij}_t$ for $1\le i,j\le 4$ and $i\neq j$ be the estimated flow from state $i$ to state $j$ between time $t$ and $t+1$. Write 
\[
Q_t(\{\lambda^{i,j}_t)=\left\{\begin{array}{cc}
						\lambda^{i,j}_t & \mbox{ if $i\neq j$}\\
						-\sum_{j\neq i} \lambda^{i,j}_t & \mbox{ if $i=j$}\end{array}\right.
\] 
(We do not write $Q^{i,j}_t$ for the flow rate to emphasize that when we vary $\lambda^{i,j}_t$, we automatically vary the diagonal element of $Q$ to make it a flow matrix, i.e. the flow matrix is a \emph{function} of the $n(n-1)$ defined flows).

We write $\hat{\lambda}^{i,j}_t$ for the trend of the flow $\lambda^{i,j}_t$ and we write $\hat{Q}_t$ for the associated trend flow matrix obtained from $\{\hat{\lambda}^{i,j}_t\}_{i,j; i\neq j}$. 

Define the trend state $\hat{x}_t \in \mathbb{R}^4$ by
\[
\hat{x}_t=x_0\exp\left(\sum_{s=0}^{t-1} \hat{Q}_s\right).
\]
By the definition of $Q_t$, the actual state is given by
\[
x_t=x_0\exp\left(\sum_{s=0}^{t-1} Q_s\right).
\]
Hence, the deviation can be decomposed into deviations between $\hat{Q}_s$ and $Q_s$, which in turn is driven by deviations between $\{\lambda^{i,j}_s\}$ and $\{\hat{\lambda}^{i,j}_s\}$. 

We make a log-linear approximation of $x$ to get 
\[
\log\left(\frac{x_t}{\hat{x}_t}\right) = \sum_{s=0}^{t-1} \sum_{i,j \quad i\neq j} \left.\frac{\partial \exp(\sum_{s=0}^{t-1} Q_s)}{\partial \lambda^{i,j}_t}\right|_{Q_s=\hat{Q}_s, s=0,\dots, t-1} \hat{\lambda}^{i,j}_t \log\left(\frac{\lambda_t^{i,j}}{\hat{\lambda}^{i,j}_t}\right)+\varepsilon_t
\]
where $\varepsilon_t$ is an error term which is quadratic in $\{\lambda^{i,j}_t\}$. Define
\[
\Delta^{i,j}_t = \sum_{s=0}^{t-1} \left.\frac{\partial \exp(\sum_{s=0}^{t-1} Q_s)}{\partial \lambda^{i,j}_t}\right|_{Q_s=\hat{Q}_s, s=0,\dots, t-1} \hat{\lambda}^{i,j}_t \log\left(\frac{\lambda_t^{i,j}}{\hat{\lambda}^{i,j}_t}\right)
\]
as the contribution of variations in the flows from $i$ to $j$ to overall variations in $x$. Let $x(k)$ be the $k^{th}$ component of $x$ and $\Delta^{i,j}_t(k)$ the $k^{th}$ component of $\Delta$. Then define the contribution of $i,j$ as
\[
\frac{Cov(\Delta^{i,j}_t(k),\log\left(\frac{x_t(k)}{\hat{x}_t(k)}\right)}{Var\left(\log\left(\frac{x_t(k)}{\hat{x}_t(k)}\right)\right)}.
\]
This expression sums to $1$ apart from the covariance between trend deviations in $x_k(t)$ and the quadratic error term $\varepsilon_t(k)$. 


\section{Relation to other papers}
Shimer looks at covariance between $\frac{\bar{f}}{x_t+\bar{f}}$ to $\frac{f}{x+f}$ once both have been detrended. It is not clear that this is a linear decomposition or where the approximation is made around. $\bar{f}$ is the long-run average, but deviation is from trend.
ah
Fujita \& Ramey: Use similar method but in two dimensions.
