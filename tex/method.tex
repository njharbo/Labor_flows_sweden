\section{Method}
\subsection{Environment and notation}
Time is given by $t\in[0,T]$. We assume the distribution data is generated by a process where all workers are identical and move between a set of labor market states
\[
\mathcal{S} = \{1,\dots,S\}
\]
according to a time-inhomogenous continuous-time Markov chain which is generated by a flow matrix $Q(t)$. We write $x(t)\in \mathbb{R}^S$ for the stochastic process of worker shares in different states.

Underthis definition, the unemployment rate  is a one-dimensional stochastic process $\frac{x_U(t)}{x_U(t)+x_P(t)+x_T(t)}$ where $x_U$ is the share of unemployed, $x_P$ is the share of permanently employed, and $x_T$ is the share of temporary employed as a percentage of all working-age people.

We write $P(t_s,t_{s+1})$ for the transition matrix between time periods $t_s$ and $t_{s+1}$. \footnote{There has been a discussion whether it is appropriate to model the labor market as a continuous time markov chain or as composed by a mininum discrete time unit such as a week. In the Appendix we redo the analysis under different assumptions. For now, we use the continuous time assumption as it allows for an easy interpretation of flow rates.}

We observe the distribution vector $x(t) \in \mathbb{R}^S$ at discrete time points
\[
0=t_0<t_1<t_2\dots <t_k = T.
\]
We also can use the panel structure of the data to derive the transition matrix 
\[
P(t_s,t_{s+1}) \quad s=1,\dots,k-1. \footnote{More precisely, we estimate $P(t_s,t_{s+1})$ using the distribution of $t_{s+1}$ states of agents observed at time $t_s$ conditional on state. Given the large number of individuals, the sampling error is small and we proceed as if the estimate is the true transition matrix.}
\]

The aim of our analysis is to use the data to
\begin{enumerate}
\item Identify $Q(t)$
\item Decompose the time variation in unemployment into changes attributable to different elements of $Q(t)$.
\end{enumerate}

\subsection{Identify $Q(t)$}
To identify $Q(t)$ $t\in[0,T]$ from $P(t_s,t_{s+1})$ $s\in\{1,\dots,k-1\}$. We use the well-known theorem from the theory of continuous-time Markov chains [referens] that
\[
P(t_s,t_{s+1})=\exp\left(\int_{t_s}^{t_{s+1}} Q(z)dz\right)
\]
where $\exp\left(\dots\right)$ is matrix exponentiation. 

To identify $Q$ we further make the assumption that $Q(t)$ is constant on each measurement interval $[t_s,t_{s+1})$. Under this assumption, we estimate $Q(t)$ by
\[
Q(t)=\frac{\log(P(t_s,t_{s+1}))}{t_{s+1}-t_s} \quad t\in[t_s,t_{s+1}); s\in\{1,\dots,k-1\}.
\]
The matrix logarithm exists and is real-valued if and only if the negative eigenvalues of $P$ in the Jordan composition has an even block size. If this is not true, $P$ is not "embeddable" which means that it cannot be the result from a continuous time markov chain with constant flow matrix. If this is the case there exist algorithms to derive an approximate generating flow matrix $Q$.

\subsection{Stationary distribution approximation}
In some cases, we can approximate the observed distribution $x(t_s)$ by the corresponding steady state distribution of $Q(t)$ for $t\in[t_{s-1},t_s)$, which we write $Q^s$.

When $Q^s$ have $S$ distinct eigenvalues, the Perron-Frobenius theorem means that it has a unique eigenvalue $\lambda_1$ which satisfies $\lambda_1=0$, and the eigenvalues $\lambda_1,\dots,\lambda_S$ satisfy
\[
Re(\lambda_S)<Re(\lambda_{S-1})<\dots < Re(\lambda_1)=0
\]
where $Re(\dot)$ gives the real part of a complex number. The eigenvector $x_s^1$ corresponding to $\lambda_1$ is the unique stationary distribution provided that $\sum x_s^1=1$ where $\sum$ denotes component-wise summation.

All eigenvalues $x_s^i$ corresponding to $\lambda_i$ with $i\ge 2$ has component sum $0$. Thus, if we decompose $x(t_s)$ into its eigenvalues we get
\[
x(t_s)=x_s^1 + \sum_{i=2}^S q^i_s x_s^i
\]
i.e. the stationary distribution has coefficient one and the others have arbitrary coefficients $q^i_s$. 

Thus, the distance of $x(t_{s+1})$ from $x_s^1$ is 
\begin{eqnarray*}
||x(t_{s+1})-x_s^1|| &=& ||x(t_s)\exp((t_{s+1}-t_s)Q^s)-x_s^1||\\
&=& ||\sum_{i=2}^S q^i_s \left(\exp((t_{s+1}-t_s)Q^s)\right) x_s^i||\\
&\le& \sum_{i=2}^S |q^i_s| ||\exp((t_{s+1}-t_s)Q^s)x_s^i||\\
&\le& exp(\lambda_2 (t_{s+1}-t_s)) \sum_{i=2}^S |q^i_s| ||x_s^i||
\end{eqnarray*}
So we see that the distance between $x(t_{s+1})$ and its corresponding steady state is bounded above by $\exp(\lambda_2(t_{s+1}-t_s))$ where $\lambda_2$ is the second largest eigenvalue of $Q^s$.\footnote{In some papers, the important term for convergence is $f+s$ where $f$ is job finding rate and $s$ is job separation rate. This is because in a two-dimensional model $\left(\begin{array}{cc} -s & s \\ f & -f\end{array}\right)$ is the transition matrix and $-(f+s)$ is the second largest eigenvalue.}

\subsection{Taylor approximation}
Insofar $\lambda_2$ is small and convergence is rapid, the sequence of observations $s(t_s)$ $s\in\{1,\dots,k-1\}$ can be closely approximated as a function only of $Q^s$ where $Q^s$ is the value $Q$ takes on the interval $[t_{s-1},t_s)$. 

So we obtain a sequence of observations 
\[
f(Q^s) \quad s\in \{1,\dots,k-1\}
\]
and want to derive how much of the variation in $f$ can be attributed to different components of $Q^s$.

We want to focus on cyclical variations in $Q^s$ and $f(Q^s)$ so we want to remove trends. Thus, we extract the trend $\bar{Q}_s$ and see how $f(Q^s)-f(\bar{Q}_s)$ is driven by the different components of $Q$. 

The natural way to do this is to do a Taylor expansion of $f(Q^s)-f(\bar{Q}_s)$ which allows us to linearly decompose. We do a log-linear transformation and obtain
\[
\log\left(\frac{f(Q^s)}{f(\bar{Q}_s)}\right) = \sum_{i,j; i\neq j} \frac{\partial f}{\partial Q_{i,j}}\frac{\bar{Q}^s_{i,j}}{f}\log\left(\frac{{Q}^s_{i,j}}{\bar{Q}^s_{i,j}}\right)+\mathcal{O}\left(||Q^s-\bar{Q}^s||^2\right)
\]
With this formulation, we can define the percentage contribution of flow matrix element $Q_{i,j}$ as 
\[
\beta_{i,j} = \frac{Cov\left(\log\left(\frac{f(Q^s)}{f(\bar{Q}_s)}\right), \frac{\partial f}{\partial Q_{i,j}}\frac{\bar{Q}^s_{i,j}}{f}\log\left(\frac{{Q}^s_{i,j}}{\bar{Q}^s_{i,j}}\right)\right)}{Var\left(\log\left(\frac{f(Q^s)}{f(\bar{Q}_s)}\right)\right)}
\]
and this will sum to 1 apart from the quadratic error term.


\subsection{Decomposition without steady-state approximation}
\subsection{Eigenvalue analysis and appropriateness of decomposition}
The convergence rate of a Markov chain to steady state can be analyzed using eigenvalue analysis. In the generic case, the flow matrix $Q_t$ (short-hand for the value $Q(t)$ takes for the interval $[t,t+s)$) has $n$ distinct eigenvalues $\lambda_1(t),\dots,\lambda_n(t)$ which satisfy
\[
\lambda_1 < \dots \lambda_{n-1}<\lambda_n = 0
\]
with corresponding eigenvectors $v_1(t),\dots,v_n(t)$. Here, $v_n(t)$ has only non-negative value and is the unique invariant probability distribution associated with $Q_t$ once we normalize its sum to $1$.

This means that $\{v_i(t)\}_{i=1}^n$ form a basis for $\mathbb{R}^n$. Thus, we can decompose the labor market state $x(t)$
\[
x(t)=\sum_{i=1}^n q_i(t)v_i(t)
\]
for some real values $q_i$. It can be shown that $q_n(t)=1$ for all $Q_t$ which means that the invariant probability distribution vector $v_n(t)$ always gets weight $1$ when we decompose the probability distribution $x(t)$ with respect to the flow matrix $Q_t$. Writing $\bar{x}(t)$ for this steady-state, we obtain
\[
x(t)=\bar{x}(t)+\sum_{i=1}^{n-1}q_i(t)v_i(t).
\]
Now, we use that if $v$ is a left eigenvector of $Q$ with eigenvalue $\lambda$, then $v$ is also an eigenvalue of $\exp(sQ)$ with eigenvalue $\exp(s\times\lambda)$. Indeed
\begin{eqnarray*}
v\exp(Q)&=&v\sum_{j=0}^\infty \frac{(sQ)^j}{j!}\\
&=&\sum_{j=0}^\infty \frac{s^j \lambda^j v}{j!}\\
&=&\exp(s\lambda)v
\end{eqnarray*}
as required. This means that we can use the decomposition to obtain
\begin{eqnarray*}
x(t+s)&=&x(t)exp(Q_t s)\\
&=&\bar{x}(t)+\sum_{i=1}^{n-1}q_i(t)v_i(t)\exp(sQ_t)\\
&=&\bar{x}(t)+\sum_{i=1}^{n-1}q_i(t)\exp(s\lambda_i)v_i(t)
\end{eqnarray*}
As $\exp(s\lambda_i)\to 0$ for $s\to\infty$ whenever $\lambda_i<0$ (i.e. for $i=1,\dots,n-1$), we obtain the classic result that $x(t+s)\to \bar{x}(t)$. Moreover, we get that that the convergence rate is determined by $\lambda_{n-1}$, the second largest eigenvalue (as all other terms become insignificant compared to the $\exp(\lambda_{n-1}s)$-term asymptotically. Thus, to analyze whether a steady-state approximation is justified we can analyze the second eigenvalue of the flow matrix.

\subsection{Eigenvalues in our data}
When we do an eigenvalue analysis of our estimated flow matrices, we get that the second largest eigenvalue is approximately $-0.02$. This means that the convergence rate is only $2\%$ monthly and $6\%$ on a quarterly basis. This means that a steady state analysis can be inappropriate with our data. (In standard two-state analysis in the US, the flow matrix is 
\[
Q=\left(\begin{array}{cc}
	  -s & s \\
	  f & -f\end{array}\right)
\]
which has a second largest eigenvalue $-(s+f)$ which has an approximate size $0.5$). [To be added: Redoing our analysis without permanent/temporary partition etc].

The main problem in the data is that the loss rate is extremely low from permanent jobs in Sweden, which means that it takes a long time for the economy to adjust to a new steady state. [Could be added here: "decompose" the causes of a large second eigenvalue].

\subsection{Decomposition without steady-state approximation}
In this section, we outline how to decompose the cyclical component of unemployment without using a steady-state approximation. 

We begin with some notation. Let $\lambda^{ij}_t$ for $1\le i,j\le 4$ and $i\neq j$ be the estimated flow from state $i$ to state $j$ between time $t$ and $t+1$. Write 
\[
Q_t(\{\lambda^{i,j}_t)=\left\{\begin{array}{cc}
						\lambda^{i,j}_t & \mbox{ if $i\neq j$}\\
						-\sum_{j\neq i} \lambda^{i,j}_t & \mbox{ if $i=j$}\end{array}\right.
\] 
(We do not write $Q^{i,j}_t$ for the flow rate to emphasize that when we vary $\lambda^{i,j}_t$, we automatically vary the diagonal element of $Q$ to make it a flow matrix, i.e. the flow matrix is a \emph{function} of the $n(n-1)$ defined flows).

We write $\hat{\lambda}^{i,j}_t$ for the trend of the flow $\lambda^{i,j}_t$ and we write $\hat{Q}_t$ for the associated trend flow matrix obtained from $\{\hat{\lambda}^{i,j}_t\}_{i,j; i\neq j}$. 

Define the trend state $\hat{x}_t \in \mathbb{R}^4$ by
\[
\hat{x}_t=x_0\exp\left(\sum_{s=0}^{t-1} \hat{Q}_s\right).
\]
By the definition of $Q_t$, the actual state is given by
\[
x_t=x_0\exp\left(\sum_{s=0}^{t-1} Q_s\right).
\]
Hence, the deviation can be decomposed into deviations between $\hat{Q}_s$ and $Q_s$, which in turn is driven by deviations between $\{\lambda^{i,j}_s\}$ and $\{\hat{\lambda}^{i,j}_s\}$. 

We make a log-linear approximation of $x$ to get 
\[
\log\left(\frac{x_t}{\hat{x}_t}\right) = \sum_{s=0}^{t-1} \sum_{i,j \quad i\neq j} \left.\frac{\partial \exp(\sum_{s=0}^{t-1} Q_s)}{\partial \lambda^{i,j}_t}\right|_{Q_s=\hat{Q}_s, s=0,\dots, t-1} \hat{\lambda}^{i,j}_t \log\left(\frac{\lambda_t^{i,j}}{\hat{\lambda}^{i,j}_t}\right)+\varepsilon_t
\]
where $\varepsilon_t$ is an error term which is quadratic in $\{\lambda^{i,j}_t\}$. Define
\[
\Delta^{i,j}_t = \sum_{s=0}^{t-1} \left.\frac{\partial \exp(\sum_{s=0}^{t-1} Q_s)}{\partial \lambda^{i,j}_t}\right|_{Q_s=\hat{Q}_s, s=0,\dots, t-1} \hat{\lambda}^{i,j}_t \log\left(\frac{\lambda_t^{i,j}}{\hat{\lambda}^{i,j}_t}\right)
\]
as the contribution of variations in the flows from $i$ to $j$ to overall variations in $x$. Let $x(k)$ be the $k^{th}$ component of $x$ and $\Delta^{i,j}_t(k)$ the $k^{th}$ component of $\Delta$. Then define the contribution of $i,j$ as
\[
\frac{Cov(\Delta^{i,j}_t(k),\log\left(\frac{x_t(k)}{\hat{x}_t(k)}\right)}{Var\left(\log\left(\frac{x_t(k)}{\hat{x}_t(k)}\right)\right)}.
\]
This expression sums to $1$ apart from the covariance between trend deviations in $x_k(t)$ and the quadratic error term $\varepsilon_t(k)$. 


\section{Relation to other papers}
Shimer looks at covariance between $\frac{\bar{f}}{x_t+\bar{f}}$ to $\frac{f}{x+f}$ once both have been detrended. It is not clear that this is a linear decomposition or where the approximation is made around. $\bar{f}$ is the long-run average, but deviation is from trend.
ah
Fujita \& Ramey: Use similar method but in two dimensions.
